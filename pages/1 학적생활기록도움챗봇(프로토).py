import streamlit as st
import json
import os
import pathlib
import toml
from langchain_core.messages import ChatMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS

# Streamlitì˜ ê¸°ë³¸ ë©”ë‰´ì™€ í‘¸í„° ìˆ¨ê¸°ê¸°
hide_github_icon = """
    <style>
    .css-1jc7ptx, .e1ewe7hr3, .viewerBadge_container__1QSob,
    .styles_viewerBadge__1yB5_, .viewerBadge_link__1S137,
    .viewerBadge_text__1JaDK{ display: none; }
    #MainMenu{ visibility: hidden; }
    footer { visibility: hidden; }
    header { visibility: hidden; }
    </style>
"""

st.markdown(hide_github_icon, unsafe_allow_html=True)

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ secrets.toml íŒŒì¼ ê²½ë¡œ ì„¤ì •
secrets_path = pathlib.Path(__file__).parent.parent / ".streamlit" / "secrets.toml"

# secrets.toml íŒŒì¼ ì½ê¸°
try:
    with open(secrets_path, "r") as f:
        secrets = toml.load(f)
except FileNotFoundError:
    st.error(f"Secrets file not found at {secrets_path}. Please ensure the file exists.")
    secrets = {}

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = secrets.get("OPENAI", {}).get("API_KEY")
if not openai_api_key:
    st.error("OpenAI API key is missing. Please ensure it is set in the secrets.toml file.")

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”."""
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "pdf_chain" not in st.session_state:
        st.session_state["pdf_chain"] = None
    if "pdf_retriever" not in st.session_state:
        st.session_state["pdf_retriever"] = None

initialize_session_state()

st.title("2024 í•™ì  Â· ìƒí™œê¸°ë¡ ë„ì›€ ì±—ë´‡ ğŸ¤–")

# íŒŒì¼ ê²½ë¡œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
file_paths = [
    "./files/2024 ì´ˆë“± í•™êµìƒí™œê¸°ë¡ë¶€ ê¸°ì¬ìš”ë ¹.pdf", 
    "./files/2024 í•™ì ì—…ë¬´ ë„ì›€ìë£Œ.pdf", 
    "./files/ì´ˆì¤‘ë“±êµìœ¡ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹).pdf", 
    "./files/ì´ˆì¤‘ë“±êµìœ¡ë²•(ë²•ë¥ )(ì œ19740í˜¸).pdf"
]
fixed_prompt_text = "íŒŒì¼ì„ ë¶„ì„í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”."
selected_model = "gpt-4o-mini"

def print_messages():
    """ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥."""
    for chat_message in st.session_state["messages"]:
        st.write(f"{chat_message.role}: {chat_message.content}")

def add_message(role, message):
    """ìƒˆë¡œìš´ ëŒ€í™” ë©”ì‹œì§€ ì¶”ê°€."""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_files(file_paths):
    """ì‚¬ì „ì— ì§€ì •ëœ íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±."""
    all_docs = []
    for file_path in file_paths:
        loader = PDFPlumberLoader(file_path)
        docs = loader.load()
        
        # ë¬¸ì„œì˜ ê° í˜ì´ì§€ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì„¤ì •
        for i, doc in enumerate(docs):
            doc.metadata = {
                "document_name": pathlib.Path(file_path).name,
                "page_number": i + 1  # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘
            }
            all_docs.append(doc)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(all_docs)
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    return vectorstore.as_retriever()

def create_chain(retriever, prompt_text, model_name):
    """ì²´ì¸ ìƒì„±."""
    llm = ChatOpenAI(model_name=model_name, temperature=0, api_key=openai_api_key)

    def chain(question):
        # Retrieve the relevant documents and pass them to the LLM along with the question
        context_docs = retriever.get_relevant_documents(question)
        context = " ".join([doc.page_content for doc in context_docs])  # Concatenate the content of relevant documents
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            SystemMessage(content=prompt_text),
            HumanMessage(content=f"{context}\n\nì§ˆë¬¸: {question}")
        ]
        output = llm.invoke(messages)
        
        # í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë„ë¡ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ì œê±°)
        if isinstance(output, ChatMessage):
            return output.content, context_docs
        else:
            return str(output), context_docs
    
    return chain

def extract_reference_from_metadata(docs):
    """ë ˆí¼ëŸ°ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜."""
    # ë¬¸ì„œ ì´ë¦„ê³¼ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
    references = []
    for doc in docs:
        references.append(f"ë¬¸ì„œ: {doc.metadata.get('document_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}, í˜ì´ì§€: {doc.metadata.get('page_number', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    return " | ".join(references)

# íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ ê¸°ëŠ¥ì„ ìƒì„±
retriever = embed_files(file_paths)
chain = create_chain(retriever, prompt_text=fixed_prompt_text, model_name=selected_model)
st.session_state["pdf_retriever"] = retriever
st.session_state["pdf_chain"] = chain

print_messages()

# ì§ˆë¬¸ ì…ë ¥ê³¼ ë‹µë³€ ìƒì„± ë²„íŠ¼
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì˜ë¬´êµìœ¡ê´€ë¦¬ìœ„ì›íšŒëŠ” ì–´ë–»ê²Œ êµ¬ì„±í•˜ë‚˜? / ì •ì›ì™¸ ê´€ë¦¬ì˜ ì ˆì°¨ë¥¼ ì•Œë ¤ì¤˜.")
generate_btn = st.button("âœ¨ ë‹µë³€ ìƒì„±")

if generate_btn and user_input:
    chain = st.session_state["pdf_chain"]
    if chain:
        with st.spinner("AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            response, context_docs = chain(user_input)
            ai_answer = response  # ì‘ë‹µì—ì„œ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            
            # ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥
            if isinstance(ai_answer, str):
                ai_answer = ai_answer.strip()  # ë¬¸ìì—´ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            
            # ë ˆí¼ëŸ°ìŠ¤ ì¶”ì¶œ
            reference = extract_reference_from_metadata(context_docs)
            
            # ì¶œë ¥ ë ˆì´ì•„ì›ƒ
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ë‹µë³€**")
                st.markdown(ai_answer)
            with col2:
                st.markdown("**ê´€ë ¨ ê·¼ê±°**")
                st.markdown(reference)

            add_message("user", user_input)
            add_message("assistant", ai_answer)
    else:
        st.error("ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
