from langchain_community.document_loaders import TextLoader
import streamlit as st
import pathlib
import toml
from langchain_core.messages import ChatMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Streamlitì˜ ê¸°ë³¸ ë©”ë‰´ì™€ í‘¸í„° ìˆ¨ê¸°ê¸°
hide_github_icon = """
    <style>
    .css-1jc7ptx, .e1ewe7hr3, .viewerBadge_container__1QSob,
    .styles_viewerBadge__1yB5_, .viewerBadge_link__1S137,
    .viewerBadge_text__1JaDK{ display: none; }
    #MainMenu{ visibility: hidden; }
    footer { visibility: hidden; }
    header { visibility: hidden; }
    </style>
"""

st.markdown(hide_github_icon, unsafe_allow_html=True)

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ secrets.toml íŒŒì¼ ê²½ë¡œ ì„¤ì •
secrets_path = pathlib.Path(__file__).parent.parent / ".streamlit" / "secrets.toml"

# secrets.toml íŒŒì¼ ì½ê¸°
try:
    with open(secrets_path, "r") as f:
        secrets = toml.load(f)
except FileNotFoundError:
    st.error(f"Secrets file not found at {secrets_path}. Please ensure the file exists.")
    secrets = {}

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = secrets.get("OPENAI", {}).get("API_KEY")
st.write(f"OpenAI API Key: {openai_api_key}")
if not openai_api_key:
    st.error("OpenAI API key is missing. Please ensure it is set in the secrets.toml file.")

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”."""
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "pdf_chain" not in st.session_state:
        st.session_state["pdf_chain"] = None
    if "pdf_retriever" not in st.session_state:
        st.session_state["pdf_retriever"] = None

initialize_session_state()

st.title("2024 í•™ì  Â· ìƒí™œê¸°ë¡ ë„ì›€ ì±—ë´‡ ğŸ¤–")

# íŒŒì¼ ê²½ë¡œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
file_paths = [
    "./files/2023_í•™ìƒë¶€_ì¢…í•©ì§€ì›ì„¼í„°_ì§ˆì˜_íšŒì‹ ì‚¬ë¡€ì§‘_utf8.txt", 
    "./files/2024_ì´ˆë“±_í•™êµìƒí™œê¸°ë¡ë¶€_ê¸°ì¬ìš”ë ¹_utf8.txt", 
    "./files/ì´ˆì¤‘ë“±êµìœ¡ë²•_ë²•ë¥ _ì œ19740í˜¸.txt", 
    "./files/ì´ˆì¤‘ë“±êµìœ¡ë²•_ì‹œí–‰ë ¹.txt",
    "./files/2024_í•™ì ì—…ë¬´_ë„ì›€ìë£Œ_utf8.txt"
]
fixed_prompt_text = "íŒŒì¼ì„ ë¶„ì„í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”."
selected_model = "gpt-4o-mini"

def add_message(role, message):
    """ìƒˆë¡œìš´ ëŒ€í™” ë©”ì‹œì§€ ì¶”ê°€."""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_files(file_paths):
    """ì‚¬ì „ì— ì§€ì •ëœ íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±."""
    all_docs = []
    for file_path in file_paths:
        try:
            loader = TextLoader(file_path, encoding="utf-8")  # ì¸ì½”ë”© ì˜µì…˜ ì¶”ê°€
            docs = loader.load()
            
            # ë¬¸ì„œì˜ ê° í˜ì´ì§€ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì„¤ì •
            for i, doc in enumerate(docs):
                doc.metadata = {
                    "document_name": pathlib.Path(file_path).name,
                    "page_number": i + 1  # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘
                }
                all_docs.append(doc)
        except Exception as e:
            st.error(f"Failed to load file {file_path}: {e}")
            continue

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(all_docs)
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    return vectorstore.as_retriever()

def create_chain(retriever, prompt_text, model_name):
    """ì²´ì¸ ìƒì„±."""
    llm = ChatOpenAI(model_name=model_name, temperature=0, api_key=openai_api_key)

    def chain(question):
        # Retrieve the relevant documents and pass them to the LLM along with the question
        context_docs = retriever.get_relevant_documents(question)
        context = " ".join([doc.page_content for doc in context_docs])  # Concatenate the content of relevant documents
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            SystemMessage(content=prompt_text),
            HumanMessage(content=f"{context}\n\nì§ˆë¬¸: {question}")
        ]
        output = llm.invoke(messages)
        
        # í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë„ë¡ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ì œê±°)
        if isinstance(output, ChatMessage):
            return output.content, context_docs  # ChatMessageì˜ content ì†ì„±ë§Œ ë°˜í™˜
        else:
            return str(output), context_docs
    
    return chain

def extract_reference_from_metadata(docs):
    """ë ˆí¼ëŸ°ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜. ë¬¸ì„œ ì´ë¦„, í˜ì´ì§€ ë²ˆí˜¸ì™€ í•¨ê»˜ í•´ë‹¹ í˜ì´ì§€ì˜ ë‚´ìš©ë„ í¬í•¨."""
    references = []
    for doc in docs:
        document_name = doc.metadata.get('document_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        page_number = doc.metadata.get('page_number', 'ì•Œ ìˆ˜ ì—†ìŒ')
        page_content = doc.page_content.strip()[:200]  # ì²« 200ìë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥
        references.append(f"ë¬¸ì„œ: {document_name}, í˜ì´ì§€: {page_number}\në‚´ìš©: {page_content}...")
    return "\n\n".join(references)

def clean_response(response):
    """ì‘ë‹µ ë¬¸ìì—´ì—ì„œ 'content=' ë¶€ë¶„ì„ ì œê±°í•˜ê³  ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜"""
    if "content=" in response:
        # 'content=' ì´í›„ì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³  ë©”íƒ€ë°ì´í„° ë¶€ë¶„ì€ ì œê±°
        cleaned_response = response.split("content=", 1)[1].split("response_metadata=", 1)[0]
        # í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œì™€ ê°œí–‰ ë¬¸ì ë“±ì„ ì ì ˆíˆ ì²˜ë¦¬
        cleaned_response = cleaned_response.replace("\\n", "\n").replace("'", "").replace("\\", "").strip()
        return cleaned_response
    return response.replace("\\n", "\n").replace("'", "").replace("\\", "").strip()

# íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ ê¸°ëŠ¥ì„ ìƒì„±
retriever = embed_files(file_paths)
chain = create_chain(retriever, prompt_text=fixed_prompt_text, model_name=selected_model)
st.session_state["pdf_retriever"] = retriever
st.session_state["pdf_chain"] = chain

# ì§ˆë¬¸ ì…ë ¥ê³¼ ë‹µë³€ ìƒì„± ë²„íŠ¼
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì˜ë¬´êµìœ¡ê´€ë¦¬ìœ„ì›íšŒëŠ” ì–´ë–»ê²Œ êµ¬ì„±í•˜ë‚˜? / ì •ì›ì™¸ ê´€ë¦¬ì˜ ì ˆì°¨ë¥¼ ì•Œë ¤ì¤˜.")
generate_btn = st.button("âœ¨ ë‹µë³€ ìƒì„±")

if generate_btn and user_input:
    chain = st.session_state["pdf_chain"]
    if chain:
        with st.spinner("AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            response, context_docs = chain(user_input)
            
            # responseë¥¼ í´ë¦°ì—…
            ai_answer = clean_response(response)
            
            # ë ˆí¼ëŸ°ìŠ¤ ì¶”ì¶œ (ë¬¸ì„œ ë‚´ìš©ê³¼ í•¨ê»˜)
            reference = extract_reference_from_metadata(context_docs)
            
            # ì¶œë ¥ ë ˆì´ì•„ì›ƒ
            st.markdown("### ë‹µë³€ ë° ê´€ë ¨ ê·¼ê±°")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ë‹µë³€**")
                st.markdown(ai_answer)  # ì˜¤ì§ contentë§Œ ì¶œë ¥ë˜ë„ë¡ ë³´ì¥
            with col2:
                st.markdown("**ê´€ë ¨ ê·¼ê±°**")
                st.markdown(reference)

            add_message("user", user_input)
            add_message("assistant", ai_answer)
    else:
        st.error("ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
